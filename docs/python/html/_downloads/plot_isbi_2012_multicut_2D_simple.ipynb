{
  "nbformat": 4,
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\nISBI 2012 Simple 2D Multicut Pipeline\n======================================\n\nHere we segment neuro data as in  :cite:`beier_17_multicut`.\nIn fact, this is a simplified version of :cite:`beier_17_multicut`.\nWe start from an distance transform watershed\nover-segmentation.\nWe compute a RAG and features for all edges.\nNext, we learn the edge probabilities\nwith a random forest classifier.\nThe predicted edge probabilities are\nfed into multicut objective.\nThis is optimized with an ILP solver (if available).\nThis results into a ok-ish learned segmentation\nfor the ISBI 2012 dataset.\n\nThis example will download a about 400 MB large zip file\nwith the dataset and precomputed results from :cite:`beier_17_multicut`\n\n\n\n\n"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# multi purpose\nimport numpy\nimport scipy\n\n# plotting\nimport pylab\n\n# to download data and unzip it\nimport os\nimport urllib.request\nimport zipfile\n\n# to read the tiff files\nimport skimage.io\nimport skimage.filters\nimport skimage.morphology\n\n# classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# needed parts of nifty\nimport nifty\nimport nifty.segmentation\nimport nifty.filters\nimport nifty.graph.rag\nimport nifty.ground_truth\nimport nifty.graph.optimization.multicut"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download  ISBI 2012:\n=====================\nDownload the  ISBI 2012 dataset \nand precomputed results form :cite:`beier_17_multicut`\nand extract it in-place.\n\n"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fname = \"data.zip\"\nurl = \"http://files.ilastik.org/multicut/NaturePaperDataUpl.zip\"\nif not os.path.isfile(fname):\n    urllib.request.urlretrieve(url, fname)\n    zip = zipfile.ZipFile(fname)\n    zip.extractall()"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Datasets:\n=================\nload ISBI 2012 raw and probabilities\nfor train and test set\nand the ground-truth for the train set\n\n"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "rawDsets = {\n    'train' : skimage.io.imread('NaturePaperDataUpl/ISBI2012/raw_train.tif'),\n    'test' : skimage.io.imread('NaturePaperDataUpl/ISBI2012/raw_test.tif'),\n}\n# read pmaps and convert to 01 pmaps\npmapDsets = {\n    'train' : skimage.io.imread('NaturePaperDataUpl/ISBI2012/probabilities_train.tif'),\n    'test' : skimage.io.imread('NaturePaperDataUpl/ISBI2012/probabilities_test.tif'),\n}\npmapDsets = {\n    'train' : pmapDsets['train'].astype('float32')/255.0,\n    'test' : pmapDsets['test'].astype('float32')/255.0\n}\ngtDsets = {\n    'train' : skimage.io.imread('NaturePaperDataUpl/ISBI2012/groundtruth.tif'),\n    'test'  : None\n}\n\ncomputedData = {\n    'train' : [{} for z in range(rawDsets['train'].shape[0])],\n    'test'  : [{} for z in range(rawDsets['test'].shape[0])]\n}"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Functions:\n===================\nFunction to compute features for a RAG\n(used later)\n\n"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "def computeFeatures(raw, pmap, rag):\n\n    uv = rag.uvIds()\n    nrag = nifty.graph.rag\n\n    # list of all edge features we fill \n    feats = []\n\n    # helper function to convert \n    # node features to edge features\n    def nodeToEdgeFeat(nodeFeatures):\n        uF = nodeFeatures[uv[:,0], :]\n        vF = nodeFeatures[uv[:,1], :]\n        feats = [ numpy.abs(uF-vF), uF + vF, uF *  vF,\n                 numpy.minimum(uF,vF), numpy.maximum(uF,vF)]\n        return numpy.concatenate(feats, axis=1)\n\n\n    # accumulate features from raw data\n    fRawEdge, fRawNode = nrag.accumulateStandartFeatures(rag=rag, data=raw,\n        minVal=0.0, maxVal=255.0, numberOfThreads=1)\n    feats.append(fRawEdge)\n    feats.append(nodeToEdgeFeat(fRawNode))\n\n    # accumulate data from pmap\n    fPmapEdge, fPmapNode = nrag.accumulateStandartFeatures(rag=rag, data=pmap, \n        minVal=0.0, maxVal=1.0, numberOfThreads=1)\n    feats.append(fPmapEdge)\n    feats.append(nodeToEdgeFeat(fPmapNode))\n\n    # accumulate node and edge features from\n    # superpixels geometry \n    fGeoEdge = nrag.accumulateGeometricEdgeFeatures(rag=rag, numberOfThreads=1)\n    feats.append(fGeoEdge)\n\n    fGeoNode = nrag.accumulateGeometricNodeFeatures(rag=rag, numberOfThreads=1)\n    feats.append(nodeToEdgeFeat(fGeoNode))\n\n    return numpy.concatenate(feats, axis=1)"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Over-segmentation, RAG & Extract Features:\n============================================\n\nCompute:\n *    Over-segmentation  with distance transform watersheds.\n *    Construct a region adjacency graph (RAG)\n *    Extract features for all edges in the graph\n *    Map the ground truth to the edges in the graph.\n      (only for the training set)\n\n\n"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "for ds in ['train', 'test']:\n    \n    rawDset = rawDsets[ds]\n    pmapDset = pmapDsets[ds]\n    gtDset = gtDsets[ds]\n    dataDset = computedData[ds]\n\n    # for each slice\n    for z in range(rawDset.shape[0]):   \n        \n        \n        data = dataDset[z]\n\n        # get raw and pmap slice\n        raw  = rawDset[z, ... ]\n        pmap = pmapDset[z, ... ]\n\n        \n\n        # oversementation\n        overseg = nifty.segmentation.distanceTransformWatersheds(pmap, threshold=0.3)\n        overseg -= 1\n        data['overseg'] = overseg\n\n        # region adjacency graph\n        rag = nifty.graph.rag.gridRag(overseg)\n        data['rag'] = rag\n\n        # compute features\n        features = computeFeatures(raw=raw, pmap=pmap, rag=rag)\n\n        data['features'] = features\n\n        # map the gt to edge\n        if ds == 'train':\n\n            # the gt is on membrane level\n            # 0 at membranes pixels\n            # 1 at non-membrane pixels\n            gtImage = gtDset[z, ...] \n\n            # local maxima seeds\n            seeds = nifty.segmentation.localMaximaSeeds(gtImage)\n\n            # growing map\n            growMap = nifty.filters.gaussianSmoothing(1.0-gtImage, 1.0)\n            growMap += 0.1*nifty.filters.gaussianSmoothing(1.0-gtImage, 6.0)\n            gt = nifty.segmentation.seededWatersheds(growMap, seeds=seeds)\n\n\n            # map the gt to the edges\n            overlap = nifty.ground_truth.overlap(segmentation=overseg, \n                                       groundTruth=gt)\n\n            # edge gt\n            edgeGt = overlap.differentOverlaps(rag.uvIds())\n            data['edgeGt'] = edgeGt\n\n\n            # plot each 14th \n            if z  % 14 == 0 :\n                figure = pylab.figure()\n                figure.suptitle('Training Set Slice %d'%z, fontsize=20)\n\n                #fig = matplotlib.pyplot.gcf()\n                figure.set_size_inches(18.5, 10.5)\n\n                figure.add_subplot(3, 2, 1)\n                pylab.imshow(raw, cmap='gray')\n                pylab.title(\"Raw data %s\"%(ds))\n\n                figure.add_subplot(3, 2, 2)\n                pylab.imshow(pmap, cmap='gray')\n                pylab.title(\"Membrane pmap %s\"%(ds))\n\n\n                figure.add_subplot(3, 2, 3)\n                pylab.imshow(nifty.segmentation.segmentOverlay(raw, overseg, 0.2, thin=False))\n                pylab.title(\"Superpixels %s\"%(ds))\n\n                figure.add_subplot(3, 2, 4)\n                pylab.imshow(seeds, cmap=nifty.segmentation.randomColormap(zeroToZero=True))\n                pylab.title(\"Partial ground truth %s\" %(ds))\n\n                figure.add_subplot(3, 2, 5)\n                pylab.imshow(nifty.segmentation.segmentOverlay(raw, gt, 0.2, thin=False))\n                pylab.title(\"Dense ground truth %s\" %(ds))\n                pylab.tight_layout()\n                pylab.show()\n\n        else:\n            data['edgeGt'] = None"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the training set:\n===========================\nWe only use high confidence boundaries.\n\n"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "dataDset = computedData[ds]\ntrainingSet = {'features':[],'labels':[]}\n\nfor ds in ['train']:\n    \n    rawDset = rawDsets[ds]\n    pmapDset = pmapDsets[ds]\n    gtDset = gtDsets[ds]\n    dataDset = computedData[ds]\n\n    # for each slice\n    for z in range(rawDset.shape[0]):   \n\n        data = dataDset[z]\n\n        rag = data['rag']\n        edgeGt = data['edgeGt']    \n        features = data['features']\n\n        # we use only edges which have\n        # a high certainty\n        where1 = numpy.where(edgeGt > 0.85)[0]\n        where0 = numpy.where(edgeGt < 0.15)[0]\n\n        trainingSet['features'].append(features[where0,:])\n        trainingSet['features'].append(features[where1,:])\n        trainingSet['labels'].append(numpy.zeros(len(where0)))\n        trainingSet['labels'].append(numpy.ones(len(where1)))\n\nfeatures = numpy.concatenate(trainingSet['features'], axis=0)\nlabels = numpy.concatenate(trainingSet['labels'], axis=0)"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the random forest (RF):\n===============================\n\n"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=200, oob_score=True)\nrf.fit(features, labels)\nprint(\"OOB SCORE\",rf.oob_score_)"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict Edge Probabilities & Optimize Multicut Objective:\n===========================================================\n\nPredict the edge probabilities with the learned\nrandom forest classifier.\nSet up a multicut objective and find the argmin\nwith an ILP solver (if available).\n\n"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "for ds in ['test']:\n    \n    rawDset = rawDsets[ds]\n    pmapDset = pmapDsets[ds]\n    gtDset = gtDsets[ds]\n    dataDset = computedData[ds]\n\n    # for each slice\n    for z in range(rawDset.shape[0]):   \n\n        \n        data = dataDset[z]\n\n        raw = rawDset[z,...]\n        pmap = pmapDset[z,...]\n        overseg = data['overseg']\n        rag = data['rag']\n        edgeGt = data['edgeGt']    \n        features = data['features']\n\n        predictions = rf.predict_proba(features)[:,1]\n\n\n        # setup multicut objective\n        MulticutObjective = rag.MulticutObjective\n\n        eps =  0.00001\n        p1 = numpy.clip(predictions, eps, 1.0 - eps) \n        weights = numpy.log((1.0-p1)/p1)\n\n\n        \n\n        objective = MulticutObjective(rag, weights)\n\n        # do multicut obtimization \n        if nifty.Configuration.WITH_CPLEX:\n            solver = MulticutObjective.multicutIlpCplexFactory().create(objective)\n        elif nifty.Configuration.WITH_GLPK:\n            solver = MulticutObjective.multicutIlpGurobiFactory().create(objective)\n        elif nifty.Configuration.WITH_GLPK:\n            solver = MulticutObjective.multicutIlpGlpkFactory().create(objective)\n        else:\n            solver = MulticutObjective.fusionMoveBasedFactory().create(objective)\n\n\n        arg = solver.optimize(visitor=MulticutObjective.verboseVisitor())\n        result = nifty.graph.rag.projectScalarNodeDataToPixels(rag, arg)\n\n\n        # plot for each 14th slice\n        if z % 14 == 0:\n            figure = pylab.figure()\n            \n            figure.suptitle('Test Set Results Slice %d'%z, fontsize=20)\n\n            #fig = matplotlib.pyplot.gcf()\n            figure.set_size_inches(18.5, 10.5)\n\n            figure.add_subplot(3, 2, 1)\n            pylab.imshow(raw, cmap='gray')\n            pylab.title(\"Raw data %s\"%(ds))\n\n            figure.add_subplot(3, 2, 2)\n            pylab.imshow(pmap, cmap='gray')\n            pylab.title(\"Membrane pmap %s\"%(ds))\n\n\n            figure.add_subplot(3, 2, 3)\n            pylab.imshow(nifty.segmentation.segmentOverlay(raw, overseg, 0.2, thin=False))\n            pylab.title(\"Superpixels %s\"%(ds))\n\n            figure.add_subplot(3, 2, 4)\n            pylab.imshow(result, cmap=nifty.segmentation.randomColormap())\n            pylab.title(\"Result seg. %s\" %(ds))\n\n            figure.add_subplot(3, 2, 5)\n            pylab.imshow(nifty.segmentation.segmentOverlay(raw, result, 0.2, thin=False))\n            pylab.title(\"Result seg. %s\" %(ds))\n            pylab.tight_layout()\n            pylab.show()"
      ],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    }
  ],
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "version": "3.5.3",
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python"
    }
  }
}